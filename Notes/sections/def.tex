\subsection{GSVD in LAPACK and JuliaX.X} \label{def}
\paragraph{Definition.} 
According to LAPACK~\cite[pp.~23--24]{anderson1999lapack},
the generalized singular value decomposition (GSVD) 
of an $m$-by-$n$ matrix $A$ and a $p$-by-$n$ matrix $B$ is given by 
the pair of factorizations: 
\begin{align} \label{eq:gsvdbylapack}
A = UC\onebytwo{0}{R}Q^T, \quad B = VS\onebytwo{0}{R}Q^T
\end{align}
where 
\begin{itemize}
\item $U$ is $m$-by-$m$, $V$ is $p$-by-$p$, 
$Q$ is $n$-by-$n$ and all three matrices are orthogonal.

\item $R$ is a $(k+\ell)$-by-$(k+\ell)$, upper triangular and 
nonsingular, $\onebytwo{0}{R}$ is $k+\ell$-by-$n$. 

\item $C$ is $m$-by-$(k+\ell)$ and $S$ is $p$-by-$(k+\ell)$, 
both are real non-negative diagonal (returned
in the arrays $\alpha$ and $\beta$), 
and $C^T C + S^T S = I_{k+\ell}$.  
$C$ and $S$ have the following detailed structures: 
\begin{enumerate}[\hspace{2em}(1)]
\item Case $m \ge k+\ell$: 
\[
                    C = \bordermatrix{ & k & \ell  \cr
                    \hfill k & I & 0 \cr
                    \hfill \ell & 0 & \Sigma_1 \cr
                    m-k-\ell & 0 & 0}, \  \ \ \
                    S = \bordermatrix{ & k & \ell \cr
                    \hfill \ell & 0 & \Sigma_2 \cr
                    \hfill p-\ell & 0 & 0},
\]
where $\Sigma_1$ and $\Sigma_2$ are diagonal matrices. 
and $\Sigma_1^2 + \Sigma_2^2 = I_{\ell}$ and $\Sigma_2$ is nonsingular. 
In this case, 
\begin{align*} 
\alpha_1 & =  \cdots = \alpha_k = 1, \,\,
  (\Sigma_1)_{ii} = \alpha_{k+i}\,\, \mbox{for $i = 1, \cdots, \ell$}, \\  
\beta_1  & = \cdots = \beta_k = 0, \,\,  
  (\Sigma_2)_{ii} = \beta_{k+i}\,\, \mbox{for $i = 1, \cdots, \ell$}.
\end{align*} 

\item Case $m < k+\ell$: 
\[
                    C = \bordermatrix{ & k & m-k & k+\ell-m  \cr
                    \hfill k & I & 0 & 0\cr
                    \hfill m-k & 0 & \Sigma_1 & 0}, \  \ \ \
                    S = \bordermatrix{ & k & m-k & k+\ell-m \cr
                    \hfill m-k & 0 & \Sigma_2 & 0\cr
                    \hfill k+\ell-m & 0 & 0 & I\cr
                    \hfill p-\ell & 0 & 0 & 0}, 
\]
where $\Sigma_1$ and $\Sigma_2$ are diagonal matrices 
and $\Sigma_1^2 + \Sigma_2^2 = I$, and $\Sigma_2$ is nonsingular. 

In this case, 
\begin{align*} 
\alpha_1 & =  \cdots = \alpha_k = 1, \,\,  
(\Sigma_1)_{ii} = \alpha_{k+i}\,\, \mbox{for $i = 1, \cdots, m-k$},\,\,
\alpha_{m+1} = \cdots \alpha_{k+\ell} = 0. \\
\beta_1 & = \cdots = \beta_k = 0, \,\,  
(\Sigma_2)_{ii} = \beta_{k+i}\,\, \mbox{for $i = 1, \cdots, m-k$}, \,\,  
\beta_{m+1} = \cdots = \beta_{k+\ell} = 1.
\end{align*} 

\Red{
Q: can two cases be consolidated into one as 
the definition by Edelman~\eqref{eq:gsvdbyedelman}?
} 

\end{enumerate}

\end{itemize}

%----------------------
\paragraph{Essential properties.} \label{properties}
\begin{enumerate}[\textit{Property} 1.]
\item $k+\ell = \mbox{rank}([A; B])$ and $\ell = \mbox{rank}(B)$.

\item 
%$C^T C $ = diag($\alpha_1^{2}, ..., \alpha_{k+\ell}^{2}$), 
%$S^T S$ = diag($\beta_1^{2}, ..., \beta_{k+\ell}^{2}$), 
%and $C^T C + S^T S = I$,
$\alpha_i$, $\beta_i \in [0, 1]$ for $i = 1,..., k+\ell$. The ratios 
\begin{equation} \label{eq:gsvdef}  
\sigma_i \equiv \alpha_i/\beta_i
\end{equation} 
are called the 
\textbf{generalized singular values} of the pair $(A, B)$, 
and are in non-increasing order. 
The first $k$ values are infinite, 
the remaining $\ell$ values are finite.
         
\item If we rewrite the GSVD ~\eqref{eq:gsvdbylapack} as 
\begin{align}
A\onebytwo{Q_1}{Q_2} = UC\onebytwo{0}{R_0}, \quad 
B\onebytwo{Q_1}{Q_2} = VS\onebytwo{0}{R_0}
\end{align}
where $Q_1$ is $n$-by-($n-k-\ell$), $Q_2$ is $n$-by-$(k+\ell)$ 
and $R_0$ is $(k+\ell)$-by-$(k+\ell)$. Then, 
\[
\mbox{null}(A)\cap \mbox{null}(B) = \mbox{span}(Q_1),
\] 
i.e., $Q_1$ is an orthonormal basis 
of the common nullspace of $A$ and $B$.
         
\item Let 
$$
X = Q\bordermatrix{ & n-k-\ell & k+\ell   \cr
                                    \hfill & I & 0 \cr
                                    \hfill & 0 & R_0^{-1} },
$$ 
then $A^TA$ and $B^TB$ are simultaenously diagonalized: 
\begin{subequations} \label{eq:simuldiag} 
\begin{align} 
X^TA^TAX & = \bordermatrix{ & n-k-\ell & k+\ell   \cr
                \hfill n-k-\ell & 0 & 0 \cr
                \hfill k+\ell & 0 & C^TC}, \label{eq:simuldiag1} \\ 
X^TB^TBX & = \bordermatrix{ & n-k-\ell & k+\ell   \cr
                \hfill n-k-\ell & 0 & 0 \cr
                \hfill k+\ell & 0 & S^TS}. \label{eq:simuldiag2} 
\end{align} 
\end{subequations} 
Thus, we know the ``non-trivial" eigenpairs of the generalized eigenvalue 
problem:
\begin{align*}
A^TAX_{i+n-k-\ell} = \lambda_{i} B^TBX_{i+n-k-\ell}
\end{align*}
for $i = 1, \cdots, k+\ell$, 
where $\lambda_i = (\alpha_i/\beta_i)^2$ 
are ``non-trivial'' eigenvalues of $(A^TA, B^TB)$. 
$X_{i+n-k-\ell}$ denotes the $(i+n-k-\ell)th$ column of $X$ 
and are the corresponding eigenvectors.
            
\item Two special cases of the GSVD:
\begin{enumerate}
\item When $B$ is square and nonsingular, the GSVD
of $A$ and $B$ is equivalent to the SVD of $AB^{-1}$:
\[
A B^{-1} = U (CS^{-1}) V^T  
\]
                
\item If the columns of $\onebytwo{A^T}{B^T}^T$ are orthonormal, then 
the GSVD of $A$ and $B$ is equivalent to the 
Cosine-Sine decomposition (CSD) of $(A^T, \ B^T)^T$:
\begin{align}
A = UCQ^T, \quad 
B = VSQ^T
\end{align}
where $U$ is $m$-by-$m$, $V$ is $p$-by-$p$ and $Q$ is $n$-by-$n$ 
and all of them are orthogonal matrices.
\end{enumerate}
\end{enumerate}

%-----------------------------

\newpage 
\subsection{GSVD in Edelman (2019)} 

In \cite{edelman2019gsvd}, the GSVD of an $m$-by-$n$ matrix $A$ 
and a $p$-by-$n$ matrix $B$ is defined as follows:
\begin{align} \label{eq:gsvdbyedelman}
A = UCH, \quad B = VSH
\end{align}
where 
\begin{itemize}
\item $U$ is $m$-by-$m$, and $V$ is a $p$-by-$p$, and both 
are orthogonal matrices. 

\item $C$ is an $m$-by-$(k+\ell)$ matrix and $S$ is 
an $p$-by-$(k+\ell)$ matrix, and  $C^T C + S^T S = I$. 
$C$ and $S$ are of the following detailed structures: 
\[
C = \bordermatrix{ & k & s & \ell-s  \cr
            \hfill k & I & 0 & 0\cr
            \hfill s & 0 & \Sigma_1 & 0\cr
            \hfill m-k-s & 0 & 0 & 0}, \  \ \ \
S = \bordermatrix{ & k & s & \ell-s \cr
            \hfill p-\ell & 0 & 0 & 0\cr
            \hfill s & 0 & \Sigma_2 & 0\cr
            \hfill \ell-s & 0 & 0 & I}, 
\]
where 
$k+\ell = \rank([A; B])$, 
$\ell = \rank(B)$,  
$s = \rank(A) + \rank(B) - \rank([A; B])$. Furthermore, 
$C$ and $S$ are stored in the arrays $\alpha$ and $\beta$
of length $k+\ell$ such that 
\begin{align*} 
\alpha_1 & = \cdots = \alpha_k = 1, \quad 
\Sigma_1 = \diag(\alpha_{k+1}, \cdots, \alpha_{k+s}), \quad 
\alpha_{k+s+1} = \cdots = \alpha_{k+\ell} = 0, \\ 
\beta_1 & = \cdots = \beta_k = 0, \quad 
\Sigma_2 = \diag(\beta_{k+1}, \cdots, \beta_{k+s}), \quad 
\beta_{k+s+1} = \cdots = \beta_{k+\ell} = 1. 
\end{align*} 
%and $\Sigma_1^2 + \Sigma_2^2 = I$.

\item $H$ is an $(k+\ell)$-by-$n$ matrix and has full row rank.

\end{itemize}

\noindent A few remarks are on order: 

\begin{enumerate} 

% \item 
% To be consistent with the 3-by-3 structures of $C$ and $S$ in 
% Eq.~\eqref{eq-def-edelman}, we combine the two cases of $C$ and $S$ 
% in Section \ref{def} into one: 
% \begin{align} \label{cs-structure-rewrite}
% C = \bordermatrix{ & k & s & \ell-s  \cr
%                     \hfill k & I & 0 & 0\cr
%                     \hfill s & 0 & \Sigma_1 & 0\cr
%                     \hfill m-k-s & 0 & 0 & 0}, \  \ \ \
% S = \bordermatrix{ & k & s & \ell-s \cr
%                     \hfill s & 0 & \Sigma_2 & 0\cr
%                     \hfill l-s & 0 & 0 & I\cr
%                     \hfill p-l & 0 & 0 & 0}
% \end{align}
% where $s = \rank(A) + \rank(B) - \rank([A; B])$. 
% Likewise, $s$ equals the number of non-zero and non-one $\alpha$ 
% or $\beta$. 
            
% Therefore, to achieve Eq. \eqref{eq-def-edelman} from Eq. \eqref{eq:gsvdbylapack}, 
% we shall perform three computations:
% \begin{itemize}
%                 \item $S$ in Eq. \eqref{eq-def-edelman} is obtained by left multiply a permutation matrix with $S$ in Eq. \eqref{cs-structure-rewrite} which moves the bottom $(p-l)$ rows to the top. 
%                 \item $V$ in Eq. \eqref{eq-def-edelman} is obtained by right multiply a permutation matrix with $V$ in Eq. \eqref{eq:gsvdbylapack} which moves the right $(p-l)$ columns to the left.                
%                 \item $H = RQ^T$. Namely, multiply $R$ and $Q^{T}$ in Eq. \eqref{eq:gsvdbylapack} to get $H$ in Eq. \eqref{eq-def-edelman} 
%             \end{itemize}
            
\item All properties in Section \ref{properties} hold true 
by the definition~\eqref{eq:gsvdbyedelman}. 
In particular, by the RQ factorization of $H$: 
$H = \onebytwo{0}{R_0}Q^T$, 
where $R_0$ is an $(k+\ell)$-by-$(k+\ell)$ upper triangular matrix 
and $Q$ is an $n$-by-$n$ orthgonal matrix, then 
\[
\mbox{null}(A) \cap \mbox{null}(B) = \mbox{span}\{Q(:,1:n-k-\ell)\}.
\] 
In addition, let
 $X = Q\bordermatrix{ & n-k-\ell & k+\ell   \cr
                       \hfill & I & 0 \cr
                       \hfill & 0 & R_0^{-1} }$, then
the  ``non-trivial" eigenvalues of the generalized eigenvalue problem 
$A^T A x = \lambda B^T B x$ are the square of the 
generalized singular values of $A$ and $B$, 
and and the last $(k+\ell)$ columns of $X$ are the corresponding 
eigenvectors.  

\item
\Red{
From the LAPACK GSVD \eqref{eq:gsvdbylapack} in Section \ref{def}, 
there is no value $s$ to determine the $s$-by-$s$ blocks 
in \eqref{eq:gsvdbyedelman}.  ... How to resolve this issue? 
} 

\Blue{It seems that in the work by Paige and Saunders 
\cite{paige1986computing} and Bai and Demmel \cite{bai1993computing}, 
there is a description of the $s$-blocks. ... need to double check}.

\end{enumerate} 

\Red{Q: should we use the GSVD definitions \eqref{eq:gsvdbylapack} 
or \eqref{eq:gsvdbyedelman} in ``JuliaX.X'', or leave as options?} 

%-------------
        
\newpage 
\subsection{GSVD in MATLAB} \label{def_mat}
In MATLAB 2019b \cite{MATLAB:2019}, the GSVD of an $m$-by-$n$ matrix $A$ 
and a $p$-by-$n$ matrix $B$ is the following:
\begin{align}  \label{eq:gsvdbymatlab} 
A = UCX^T, \quad B = VSX^T
\end{align}
where 
\begin{itemize}
\item $U$ is $m$-by-$m$, $V$ is $p$-by-$p$ and both matrices are orthogonal.

\item $X$ is an $n$-by-$q$ matrix, where $q = \min\{m + p, n\}$. 

\item $C$ is $m$-by-$q$, 
$S$ is $p$-by-$q$ and both matrices are nonnegative diagonal.

The nonzero elements of $S$ are always on its main diagonal. 
The nonzero elements of $C$ are on the diagaonl 
$\mbox{diag}(C, \max(0,q-m))$. If $m \geq q$, this is the main diagonal of
$C$. 

Both $C$ and $S$ are nonnegative and $C^T C + S^T S = I$. 
If $q > m$, the rightmost $m$-by-$m$ block of $C$ is diagonal. 
Otherwise, nonzero elements are on the main diagonal of $C$.

Furthermore, $C^T C $ = diag($\alpha_1^{2}, \cdots, \alpha_q^{2}$), 
$S^T S$ = diag($\beta_1^{2}, \cdots, \beta_q^{2}$), 
where $\alpha_i$, $\beta_i \in [0, 1]$ for $i = 1,\cdots, q$. 
The ratios $\alpha_i/\beta_i$ are called the 
{\em generalized singular values} of the pair $(A, B)$ 
and are in non-decreasing order.

\end{itemize}
The following structures of $C$ and $S$ are not explicitly 
documented in MATLAB, but observed by the author.
        \begin{enumerate}
            \item $m + p \geq n$, thus $q = n$:
                \begin{enumerate}
                    \item $n > m, n \leq p$:
                        \begin{displaymath}
                            C = \bordermatrix{ & n-m & m   \cr
                            \hfill m & 0 & \Sigma_1}, \  \ \ \
                            S = \bordermatrix{ & n  \cr
                            \hfill n & \Sigma_2 \cr
                            \hfill p-n & 0 }
                         \end{displaymath}
                         
                        where $\Sigma_1$ = diag($\alpha_{n-m+1}, \cdots, \alpha_{n}$) and $\Sigma_2$ = diag($\beta_1, \cdots, \beta_n$).
                    \item $n \leq m, n > p$:
                        \begin{displaymath}
                            C = \bordermatrix{ & n  \cr
                            \hfill n & \Sigma_1 \cr
                            \hfill m-n & 0 }, \ \ \ \
                            S = \bordermatrix{ & p & n-p   \cr
                            \hfill p & \Sigma_2 & 0}
                        \end{displaymath}
                        
                        where $\Sigma_1$ = diag($\alpha_1, \cdots, \alpha_{n}$) and $\Sigma_2$ = diag($\beta_1, \cdots, \beta_p$).
                    \item $n \leq m, n \leq p$:
                        \begin{displaymath}
                            C = \bordermatrix{ & n  \cr
                            \hfill n & \Sigma_1 \cr
                            \hfill m-n & 0 }, \ \ \ \
                            S = \bordermatrix{ & n  \cr
                            \hfill n & \Sigma_2 \cr
                            \hfill p-n & 0 }
                        \end{displaymath}
                        
                        where $\Sigma_1$ = diag($\alpha_1, \cdots, \alpha_{n}$) and $\Sigma_2$ = diag($\beta_1, \cdots, \beta_n$).
                    
                    \item $n > m, n > p$:
                        \begin{displaymath}
                            C = \bordermatrix{ & n-m & m   \cr
                            \hfill m & 0 & \Sigma_1}, \  \ \ \
                            S = \bordermatrix{ & p & n-p   \cr
                            \hfill p & \Sigma_2 & 0}
                        \end{displaymath}
                        
                        where $\Sigma_1$ = diag($\alpha_{n-m+1}, \cdots, \alpha_{n}$) and $\Sigma_2$ = diag($\beta_1, \cdots, \beta_p$).
                \end{enumerate}
            \item $m + p < n$, thus $q = m+p$:
                \begin{displaymath}
                    C = \bordermatrix{ & p & m   \cr
                    \hfill m & 0 & \Sigma_1}, \  \ \ \
                    S = \bordermatrix{ & p & m   \cr
                    \hfill p & \Sigma_2 & 0}
                \end{displaymath}
                where $\Sigma_1$ = diag($\alpha_{p+1}, \cdots, \alpha_{p+m}$) and $\Sigma_2$ = diag($\beta_1, \cdots, \beta_p$).
        \end{enumerate}
        
\noindent A few remarks are in order: 
\begin{enumerate} 
\item The $n\times q$ matrix $X$ cannot be guaranteed 
to be of full rank $q$. 

\item ``The matrix $X$ has full rank if and only if the matrix 
$[A;  B]$ has full rank. 
In fact, the SVD of $X$ and the condition number of $X$ 
are equal to the SVD of $[A; B]$ and the condition number of $[A; B]$, 
respectively.''  

\item The generalized singular values (gsvs) defined
in \eqref{eq:gsvdbymatlab} could be different from the ones 
defined in \eqref{eq:gsvdef}, 
see Examples~\ref{eg:case1b} and~\ref{eg:case2b}. 

\item By the definition~\eqref{eq:gsvdbymatlab}, we have  
the factorizations of $A^TA$ and $B^TB$: 
\begin{equation} \label{eq:factAB}
A^T A = X C^T C X^T, \quad
B^T B = X S^T S X^T. 
\end{equation} 
However, since $X$ is not guaranteed to be nonsingular, 
The factorization \eqref{eq:factAB} is \Red{not} the simultaneous
diagonalization of $(A^TA, B^TB)$ unless $X$ is nonsingular. 
This implies that in general, there is \Red{no connection}
between MATLAB's generalzied singular values (and singular vectors) 
and the ``non-trivial'' eigenpairs of $(A^TA, B^TB)$. 
\Red{See Examples~\ref{eg:case1b} and~\ref{eg:case2b}.}

\Red{Meanwhile, \textit{Property} 5 is true given this definition,....
(a) holds but (b) needs to be verified.}

\item MATLAB's GSVD \eqref{eq:gsvdbymatlab}  is
also different from the one defined in 
Golub and Van Loan~\cite[pp.~309]{golub2013matrix}, 
see \eqref{eq:gsvdbyvanloan} below. 

\Red{MATLAB manual cites Golub and Van Loand, third edition, 1996...
check the definition in the third edition}  

\item \Red{There are two examples on the MATLAB GSVD on
MATLAB's website. are we getting the same results? (looks like
all full column rank.}  
        
\end{enumerate} 

\Red{Q: should we communicate with MATLAB about 
inproperly defined GSVD \eqref{eq:gsvdbymatlab}? how?} 

\newpage 
\subsection{GSVD in Golub and Van Loan}
In Golub and Van Loan (4th edition)~\cite[pp.~309]{golub2013matrix}, 
given an $m$-by-$n$ matrix $A$ and a $p$-by-$n$ matrix $B$ with 
$m \ge n$ and $r = \rank([A; B])$, the GSVD of $A$ and $B$ is:
\begin{align} \label{eq:gsvdbyvanloan} 
A = UCX^{-1}, \quad B = VSX^{-1}  
\end{align}
where
\begin{itemize}
\item $U$ is an $m$-by-$m$ orthogonal matrix.

\item $V$ is a $p$-by-$p$ orthogonal matrix.

\item $C$ and $S$ are $m$-by-$n$ and $p$-by-$n$: 
\[
            C = \bordermatrix{ & q & r-q & n-r  \cr
            \hfill q & I & 0 & 0\cr
            \hfill r-q & 0 & \Sigma_1 & 0\cr
            \hfill m-r & 0 & 0 & 0}, \quad
            S = \bordermatrix{ & q & r-q & n-r \cr
            \hfill q & 0 & 0 & 0\cr
            \hfill r-q & 0 & \Sigma_2 & 0\cr
            \hfill p-r & 0 & 0 & 0}
\]
where $q = \max\{r-p, 0\}$. 
The diagonal elements of $C$ and $S$ are stored in the arrays
$\alpha$ and $\beta$: 
\begin{align*} 
\alpha_1 & = \cdots = \alpha_q = 1,\,\, 
\Sigma_1 = \mbox{diag}(\alpha_{q+1}, \cdots, \alpha_r), \\
\beta_1 & = \cdots = \beta_q = 0, \,\, 
\Sigma_2 = \mbox{diag}(\beta_{q+1}, \cdots, \beta_r)
\end{align*}
and $\Sigma_1^2 + \Sigma_2^2 = I$.

\item $X$ is an $n$-by-$n$ nonsingular matrix. 
\end{itemize}
\Red{Q: why there is no need to have two different cases for 
$C$ and $S$ as in LAPACK definition \eqref{eq:gsvdbylapack}?}  

\bigskip 
\noindent A few remarks are in order: 
\begin{enumerate} 
\item This definition is due to Van Loan \cite{van1976generalizing}. 
It holds all properties in Section \ref{properties} 
Specifically, for \textit{Property} 3, by 
$A(X_1, X_2) = AX = UC = U(C_1, \, 0)$ and 
$B(X_1, X_2) = BX = VS = V(S_1, \, 0)$, then we have
$\mbox{null}(A)\cap \mbox{null}(B) = \mbox{span}(X_2)$, 
although in this case, $X_2$ is not an orthonormal basis.
            
\item The generalized singular value are elements of the 
set $\mu(A,B) = \{\alpha_i/\beta_i \mid i=1, \cdots, r\}$. 
            
\item $\mbox{rank}([A; B])$ is the number of ``non-trivial???'' diagonal 
      entries of $C$ and $S$.
            
\item By the definition~\eqref{eq:gsvdbyvanloan}, $A^TA$ and $B^T B$
are simultaneously diagonalized: 
\begin{align*}
X^TA^TAX = C^TC, \quad 
X^TB^TBX = S^TS,
\end{align*}
Therefore, the first $r$ quotients of the diagonal entries 
of $C^TC$ and $S^TS$ are the ``non-trivial" eigenvalues of 
the matrix pairs $(A^TA, B^T B)$, and the first $r$ columns 
of $X$ are the corresponding eigenvectors.

\item \Red{Note: MATLAB GSVD \eqref{eq:gsvdbymatlab} is also 
not in line with the GSVD \eqref{eq:gsvdbyvanloan}!}
\end{enumerate} 

%------------------------------------------------------

\newpage 
\subsection{Examples}
We now illustrate our definition \ref{def} and that of MATLAB's discussed 
in Section \ref{def_mat} with matrices of small size. Depending on the structures of $C$ 
and $S$ documented in Section \ref{def}, we devise four pairs: 
Examples 1 and 2 are contained in ``case (1) ($m \geq k+\ell$)'',
while Examples 3 and 4 fall into ``case (2) ($m < k+\ell$)''.   

\newpage
\begin{example} \label{eg:case1a} 
{\rm 
Consider a 5-by-4 matrix $A$ and a $3$-by-$4$ matrix $B$:
    \begin{equation*}
        A = \begin{bmatrix} 
            1 & 2 & 3 & 0\\
            5 & 4 & 2 & 1\\
            0 & 3 & 5 & 2\\
            2 & 1 & 3 & 3\\
            2 & 0 & 5 & 3
        \end{bmatrix}, \quad
        B = \begin{bmatrix}
            1 & 0 & 3 & -1 \\
            -2 & 5 & 0 & 1 \\
            4 & 2 & -1 & 2
        \end{bmatrix}
    \end{equation*}
where $\mbox{rank}([A; B]) = 4$ and $\mbox{rank}(B) = 3$.

\begin{enumerate}[(1).]
\item The LAPACK GSVD \eqref{eq:gsvdbylapack} computed by ``JuliaGSVD'': 
            
$k = 1$ and $\ell = 3$. Since $m = 5 \geq k+\ell = 1+3$, 
$C$ and $S$ are of the form in ``case (1)'':
\begin{equation*}
C = \begin{bmatrix}
                    1.0 & 0.0      & 0.0      & 0.0  \\
                    0.0 & 0.894685 & 0.0      & 0.0  \\  
                    0.0 & 0.0      & 0.600408 & 0.0  \\  
                    0.0 & 0.0      & 0.0      & 0.27751 \\
                    0.0 & 0.0      & 0.0      & 0.0    
                \end{bmatrix} , \quad
                S = \begin{bmatrix}
                    0.0 & 0.446698 & 0.0      & 0.0   \\  
                    0.0 & 0.0      & 0.799694 & 0.0     \\
                    0.0 & 0.0      & 0.0      & 0.960723
                \end{bmatrix}
            \end{equation*}
The generalized singular values computed are 
\[ 
\texttt{Inf},\quad 
2.0028872436786482, \quad 
0.7507971450334572, \quad
0.2888559753309598.
\] 
The computed orthogonal matrices $U$, $V$, $Q$, and the $R$ matrix are: 
\begin{align*}
                U &= \begin{bmatrix}
                 -0.060976  & -0.446679  & -0.448921 & -0.482187 & -0.602266 \\
                  0.0904806 & -0.867093  &  0.416172 &  0.115882 &  0.230944 \\
                 -0.481907  & -0.212508  & -0.636747 &  0.477322 &  0.298869 \\
                 -0.523214  & 0.0347528  &  0.410748 &  0.420777 & -0.615851 \\
                 -0.69434   & 0.0475385  &  0.226075 & -0.590913 &  0.339624
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                 -0.804633 & -0.328486 & -0.494634 \\
                 -0.288044 & -0.512512 &  0.808927 \\
                 -0.519227 &  0.793365 &  0.317765
                \end{bmatrix} \\
                Q &= \begin{bmatrix}
                  0.214542 &   0.484366  &  0.833941 &  -0.15461 \\
                  0.259709 &   0.413752  & -0.147691 &   0.85997 \\ 
                 -0.361334 &   0.767117  & -0.413972 & -0.331052 \\
                 -0.86946  & -0.0756949  &  0.333702 &  0.356304
                \end{bmatrix} \\
                R &= \begin{bmatrix}
                 5.74065 & -7.07986      & 0.125979 &    -0.316232 \\
                 0.0     & -7.96103      & -2.11852 &     -2.98601 \\
                 0.0     & -4.44089e-16  & 5.72211  &     -0.43623 \\
                 0.0     &  1.33227e-15  & -8.88178e-16 &  5.66474     
                \end{bmatrix} 
            \end{align*}
The residual norms are
\begin{center}
\begin{tabular}{c||c} \hline
$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{C}\tilde{R}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.35988438508439907 \\ \hline
$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{S}\tilde{R}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.45714285714285713 \\ 
\hline
\end{tabular}
\end{center}

\item 
By GSVD in Julia 1.3 (``{\tt svd(A,B)}''), we have $k = 1$ and $\ell = 3$. 
$D1$ and $D2$ (equivalent to $C$ and $S$ 
in LAPACK GSVD \eqref{eq:gsvdbylapack}) are:
\begin{align*}
D1 = \begin{bmatrix}
      1.0 & 0.0 & 0.0 & 0.0 \\
      0.0 & 0.894685 & 0.0 & 0.0 \\
      0.0 & 0.0 & 0.600408 & 0.0 \\   
      0.0 & 0.0 & 0.0 & 0.27751 \\
      0.0 & 0.0 & 0.0 & 0.0
    \end{bmatrix} , \ \ \ \
     D2 = \begin{bmatrix}
      0.0 & 0.446698 & 0.0 & 0.0 \\     
      0.0 & 0.0 & 0.799694 & 0.0 \\  
      0.0 & 0.0 & 0.0 & 0.960723
    \end{bmatrix}
\end{align*}
The computed orthogonal matrices $U$, $V$, $Q$, the $R$ are
\begin{align*}
    U &= \begin{bmatrix}
      -0.060976 &  -0.446679  & -0.448921 &  0.482187 & -0.602266 \\
       0.0904806 & -0.867093  &  0.416172 & -0.115882 &  0.230944 \\
      -0.481907 &  -0.212508  & -0.636747 & -0.477322 &  0.298869 \\
      -0.523214  &  0.0347528 &  0.410748 & -0.420777 & -0.615851 \\
      -0.69434  &   0.0475385 &  0.226075 &  0.590913 &  0.339624
    \end{bmatrix} \\ 
    V &= \begin{bmatrix}
      -0.804633 & -0.328486 & 0.494634 \\
      -0.288044 & -0.512512 & -0.808927 \\
      -0.519227 &  0.793365 & -0.317765
    \end{bmatrix} \\ 
    Q &= \begin{bmatrix}
       0.214542 & 0.484366 & -0.833941 &  0.15461 \\ 
       0.259709 &  0.413752 & 0.147691 & -0.85997 \\
      -0.361334 &  0.767117 &   0.413972 & 0.331052 \\
      -0.86946 & -0.0756949 & -0.333702 & -0.356304
    \end{bmatrix} \\ 
    R0 &= \begin{bmatrix}
      5.74065 & -7.07986 & -0.125979 & 0.316232 \\
      0.0  &    -7.96103 &  2.11852 &  2.98601 \\ 
      0.0  &     0.0    &  -5.72211 &  0.43623 \\ 
      0.0  &     0.0    &   0.0     &  5.66474 
    \end{bmatrix} 
\end{align*}
The residual norms
\begin{center}
\bgroup
\def\arraystretch{2}% 
\begin{tabular}{| m{0.25\textwidth}|| c |} \hline
$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{D1}\tilde{R0}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.35988438508439907 \\ \hline
$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{D2}\tilde{R0}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.45714285714285713 \\ 
\hline
\end{tabular}
\egroup
\end{center}

\item The MATLAB GSVD \eqref{eq:gsvdbymatlab} computed by ``{\tt gsvd(A,B)}'': 
        
Since $m + p  =  5 + 3 > n = 4$, $m = 5 > n = 4$ and $p=3 < n = 4$, 
the structures of $C$ and $S$ are of is the 
``case 1.(b)'' in Section \ref{def_mat}: 
\begin{equation*}
                C = \begin{bmatrix}
                    0.2775  &       0  &       0    &     0 \\
                         0  &  0.6004  &       0    &     0 \\
                         0  &       0  &  0.8947    &     0 \\
                         0  &       0  &       0    & 1.0000 \\
                         0  &       0  &       0    &     0
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                    0.9607 &        0 &        0    &  0 \\
                         0 &   0.7997 &        0    &  0 \\
                         0 &        0 &   0.4467    &  0 
                \end{bmatrix}
            \end{equation*}
Consequently, 
the generalized singular values computed are: 
\[
0.2889, \quad
0.7508, \quad
2.0029,  \quad 
\texttt{Inf}. 
\] 
The computed $U$, $V$ and $X$ matrix are
\begin{align*}
                U &= \begin{bmatrix}
                    0.4822 &  -0.4489 &  -0.4467 &  -0.0610 &  -0.6023 \\
                   -0.1159 &   0.4162 &  -0.8671 &   0.0905 &   0.2309 \\
                   -0.4773 &  -0.6367 &  -0.2125 &  -0.4819 &   0.2989 \\
                   -0.4208 &   0.4107 &   0.0348 &  -0.5232 &  -0.6159 \\
                    0.5909 &   0.2261 &   0.0475 &  -0.6943 &   0.3396
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                    0.4946 &  -0.3285 &  -0.8046 \\
                   -0.8089 &  -0.5125 &  -0.2880 \\
                   -0.3178 &   0.7934 &  -0.5192
                \end{bmatrix} \\
                X &= \begin{bmatrix}
                    0.8758 &  4.8394  & -5.1611 & -2.0437 \\
                   -4.8715 & -1.2203  & -5.5489 & -1.7290 \\
                    1.8753 & -2.2244  & -4.2415 & -7.4528 \\
                   -2.0184 &  1.7541  & -1.1683 & -4.5260
                \end{bmatrix}
            \end{align*}
The residual norms are 
\begin{center}
\begin{tabular}{c||c} \hline
$res_{A} = \frac{\Vert A - \tilde{U}\tilde{C}\tilde{X}^{T}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.5222 \\ \hline
$res_{B} = \frac{\Vert B - \tilde{V}\tilde{S}\tilde{X}^{T}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 1.3036 \\ 
			\hline
\end{tabular}
\end{center}

\item \textbf{Findings}

\begin{enumerate} 
\item The generalized singular values returned by 
``LAPACK-GSVD'', ``GSVD-Julia1.3'' and ``MATLAB-GSVD'' 
are the same, but are in different order. 

\item All quantities computed by 
``LAPACK-GSVD'' and ``GSVD-Julia1.3''
are the same, up to a sign difference. 

\item The matrix $X$ produced by MATLAB-GSVD is non-singular.

\item The eigenvalues of $(A^TA, B^T B)$ computed by MATLAB's function
{\tt eig(A'*A, B'*B)} are 
\[
0.08343777448439993, \quad
0.5636963529903901, \quad 
4.011557310890648, \quad 
\texttt{Inf}.
\]
The square roots are  
\[
0.2888559753309596, \quad
0.7507971450334572, \quad
2.002887243678647, \quad
\texttt{Inf}. 
\]
These values are equal to the gsvs computed by 
LAPACK-GSVD and MATLAB-GSVD. 

{Note:  the ``inf'' eigenvalue is due to the fact $B^T B$ 
is rank deficient.}  

\item The eigenvalues of $(A^TA, B^T B)$ computed by 
MATLAB function {\tt dsygvic(n, A'*A, B'*B, tol)}\footnote{  
http://cmjiang.cs.ucdavis.edu/xsygvic.html} 
are 
\[
4.0116, \quad
0.0834, \quad 
0.5637.
\]
The square roots are  
\[
2.0029, \quad
0.2889,\quad
0.7508. 
\]
\end{enumerate} 

\end{enumerate} 

}\end{example} 

\newpage
\begin{example} \label{eg:case1b} 
{\rm 
Consider a 3-by-4 matrix $A$ and a $4$-by-$4$ matrix $B$ 
but with rank deficiency:
        \begin{equation*}
            A = \begin{bmatrix}
                1 & 2 & 1 & 0\\
                2 & 3 & 1 & 1\\
                3 & 4 & 1 & 2\\
            \end{bmatrix}, \ \ \ \
            B = \begin{bmatrix}
                4 & 5 & 1 & 3 \\
                5 & 6 & 1 & 4 \\
                6 & 7 & 1 & 5 \\
                7 & 1 & -6 & 13
            \end{bmatrix}
        \end{equation*}
where $\mbox{rank}([A; B]) = 2$ and $\mbox{rank}(B) = 2$.

\begin{enumerate}[(1).]

\item The LAPACK-GSVD \eqref{eq:gsvdbylapack} computed by ``JuliaGSVD'': 
            
$k = 0$ and $\ell = 2$.  Since $m = 3 > k+\ell = 0 + 2$, 
the structure of $C$ and $S$ is ``case 1'':
\begin{equation*}
                C = \begin{bmatrix}
                     0.476231 & 0.0  \\     
                     0.0      & 0.0697426 \\
                     0.0      & 0.0      
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                     0.87932 & 0.0    \\  
                     0.0     & 0.997565 \\
                     0.0     & 0.0  \\   
                     0.0     & 0.0   
                \end{bmatrix}
            \end{equation*}
Consequently, the computed gsvs are 
\[ 
0.5415903238738987, \quad 
0.06991284853891487.
\] 
The computed matrices $U$, $V$, $Q$ and $R$ are: 
\begin{align*}
                U &= \begin{bmatrix}
                 -0.409031 &  0.816105 & -0.408248 \\
                 -0.56342  &  0.126058 &  0.816497 \\
                 -0.71781  & -0.563988 & -0.408248
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                 -0.472375 & -0.0876731 & -0.390874  & -0.785107  \\
                 -0.55599  & -0.135916  & -0.53894   &  0.618017  \\
                 -0.639606 & -0.184159  &  0.745532  &  0.0342253 \\
                  0.242159 & -0.969498  & -0.0307137 & -0.0221441
                \end{bmatrix} \\
                Q &= \begin{bmatrix}
                 -0.436701 & -0.689898 &  0.299328  &  0.493696 \\
                  0.563299 &  0.126599 &  0.793024  &  0.194368 \\
                 -0.689898 &  0.436701 &  0.493696  & -0.299328 \\
                 -0.126599 &  0.563299 & -0.194368  &  0.793024
                \end{bmatrix} \\
                R &= \begin{bmatrix}
                 0.0 & 0.0 & -12.2133       & -8.28663 \\
                 0.0 & 0.0 &   3.55271e-15  & -18.1154       
                \end{bmatrix}
            \end{align*}

We tested residues of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{C}, \tilde{S}$ and $\tilde{R}$.

\begin{center}
\begin{tabular}{c||c} \hline
$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{C}\tilde{R}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.6172649988387877 \\ \hline
$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{S}\tilde{R}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.40979208210904405 \\ 
\hline
\end{tabular}
\end{center}

           
\item MATLAB GSVD \eqref{eq:gsvdbymatlab} computed by ``{\tt gsvd(A,B)}'': 
        
Since $m + p  = 3 + 4 > n$=4, $m=3 < n = 4$ and $p=4 = n = 4$, 
the structures of $C$ and $S$ should be the same as case 1(a) of
MATLAB GSVD \eqref{eq:gsvdbymatlab} as follows: 
            \begin{equation*}
                C = \begin{bmatrix}
                     0  &  0.0460  &       0  &       0 \\ 
                     0  &       0  &  0.6490  &       0 \\
                     0  &       0  &       0  &  0.9946
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                    1.0000  &       0  &       0  &       0 \\
                         0  &  0.9989  &       0  &       0 \\
                         0  &       0  &  0.7608  &       0 \\
                         0  &       0  &       0  &  0.1039 
                \end{bmatrix}
            \end{equation*}
Four computed generalized singular values are 
\[
0, \quad
0.0460, \quad
0.8531, \quad
9.5769.
\] 
The computed orthogonal matrices $U$, $V$ and the $X$ matrix are:
 \begin{align*}
                U &= \begin{bmatrix}
                    0.0438  &  0.0710  &  0.9965 \\
                   -0.7618  & -0.6430  &  0.0793 \\
                    0.6464  & -0.7626  &  0.0259
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                    0.0621  &  0.0228  & -0.8563 &   0.5121 \\
                   -0.1574  &  0.3650  & -0.4722 &  -0.7868 \\
                   -0.4326  &  0.8097  &  0.1962 &   0.3445 \\
                    0.8855  &  0.4589  &  0.0720 &  -0.0075
                \end{bmatrix} \\
                X &= \begin{bmatrix}
                    3.0643 &   9.9974 &  -5.3968 &  1.2397 \\
                   -2.7768 &   8.4399 &  -7.4530 &  2.3475 \\
                   -5.8412 &  -1.5575 &  -2.0562 &  1.1078 \\
                    8.9055 &  11.5549 &  -3.3406 &  0.1319 
                \end{bmatrix} 
            \end{align*}

We checked the residues of $A$ and $B$ with the computed $\tilde{U}, \tilde{V}, \tilde{X}, \tilde{C}$ and $\tilde{S}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert A - \tilde{U}\tilde{C}\tilde{X}^{T}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 5.5139 \\ \hline
				$res_{B} = \frac{\Vert B - \tilde{V}\tilde{S}\tilde{X}^{T}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 1.0600 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}
       
\item \textbf{Findings}

\begin{enumerate}
\item The matrix $X$ in MATLAB GSVD \eqref{eq:gsvdbymatlab} 
      is singular, with rank 2.

\item Neither do the diagonal entries of $C$ and $S$ nor the generalize 
singular values produced in 
LAPACK GSVD \eqref{eq:gsvdbylapack} and MATLAB GSVD \eqref{eq:gsvdbymatlab}  
bear any resemblance in terms of the number of gsvs 
and their numerical values. 

\item The eigenvalues of $(A^TA, B^T B)$ computed by 
MATLAB's function {\tt eig(A'*A,B'*B)} are
\[
-0.035807289211371204, \quad
{\bf 0.004887806390825194}, \quad
0.12085659170178971, \quad
{\bf 0.29332007891383427}. 
\]
The square roots are 
\[
0.0 + 0.1892281406434339\texttt{im}, \quad
{\bf 0.06991284853891447}, \quad
0.3476443465695792, \quad
{\bf 0.5415903238738985}. 
\]
Among these values, eigenvalues 
{\bf 0.06991284853891447} and {\bf 0.5415903238738985}  
are found in the computed gsvs of LAPACK GSVD. 

\item \Red{Note: In this case, two of four eigenvalues
computed by MATLAB ``{\tt eig}'' are spurious ones. 
This is caused by the fact that  
the pencil $A^T A - \lambda B^T B$ is singular, i.e., 
$A^T A$ and $B^T B$ have a non-trivial common null space. 
The function ``dsygvic.m'' should return only 
two ``corrected'' eigenvalues.} 

The computation of the eigenvalues of $(A^TA, B^T B)$ by 
MATLAB's function {\tt dsygvic(n, A'*A, B'*B, tol)} caused an exception, the error message is
\Red{Singular pencil, exit}.

\end{enumerate} 

\item 
By GSVD in Julia 1.3 ({\tt svd(A, B)}), we have $k = 0$ and $\ell = 2$. 
$D1$ and $D2$ (equivalent to $C$ and $S$ in the proposed version) are:
 \begin{align*}
    D1 = \begin{bmatrix}
      0.476231 & 0.0 \\      
      0.0 & 0.0697426 \\
      0.0 & 0.0      
    \end{bmatrix}, \ \ \ \
    D2 = \begin{bmatrix}
      0.87932 & 0.0 \\     
      0.0 & 0.997565 \\
      0.0 & 0.0 \\
      0.0 & 0.0  
    \end{bmatrix}
 \end{align*} 
The computed orthogonal matrices $U$, $V$, $Q$, the $R0$ matrix (equivalent to $R$ in the proposed version) are: 
\begin{align*}
    U &= \begin{bmatrix}
      0.409031 & 0.816105 & -0.408248 \\
      0.56342 & 0.126058 & 0.816497 \\
      0.71781 & -0.563988 & -0.408248
    \end{bmatrix} \\
   V &= \begin{bmatrix}
   0.472375 & -0.0876731 & -0.390874 & -0.785107 \\ 
   0.55599 & -0.135916 & -0.53894 & 0.618017 \\ 
   0.639606 & -0.184159 & 0.745532 & 0.0342253 \\
  -0.242159 & -0.969498 & -0.0307137 & -0.0221441
    \end{bmatrix} \\
  Q &= \begin{bmatrix}
  -0.436701 & -0.689898 & -0.299328 & 0.493696 \\
   0.563299 & 0.126599 & -0.793024 & 0.194368 \\
  -0.689898 & 0.436701 & -0.493696 & -0.299328 \\
  -0.126599 & 0.563299 & 0.194368 & 0.793024
    \end{bmatrix} \\
  R0 &= \begin{bmatrix}
  0.0 & 0.0 & -12.2133  & 8.28663 \\
  0.0 & 0.0 & 0.0 & -18.1154 
    \end{bmatrix}    
\end{align*}
\Red{All these quantities are essentially (up to a sign) the same
with JuliaGSVD.}  

Still, we tested residuals of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{D1}, \tilde{D2}$ and $\tilde{R0}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{D1}\tilde{R0}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.5068000688771875 \\ \hline
				$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{D2}\tilde{R0}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.5689371432283518 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}

\end{enumerate} 
} 
\end{example} 


\newpage
\begin{example} \label{eg:case2a} 
{\rm 
Let $A$ be a 3-by-4 matrix and $B$ be a $4$-by-$4$ matrix:
        \begin{equation*}
            A = \begin{bmatrix}
                1 & 4 & 1 & 0\\
                5 & 3 & 1 & 1\\
                3 & 0 & 1 & 2\\
            \end{bmatrix}, \ \ \ \
            B = \begin{bmatrix}
                4 & 5 & 1 & 3 \\
                -2 & 0 & 1 & 4 \\
                3 & 2 & 1 & -5 \\
                1 & 1 & -6 & 3
            \end{bmatrix}
\end{equation*}

\begin{enumerate}[(1).]
        \item The LAPACK GSVD \eqref{eq:gsvdbylapack} computed by ``JuliaGSVD'': 
        
            $k = 0$ and $\ell = 4$. Since $m = 3$ and $m < k+\ell$, $C$ and $S$ should be contained in case (2) in Section \ref{def}:
            
            \begin{equation*}
                C = \begin{bmatrix}
                 0.99144 & 0.0      & 0.0      & 0.0 \\
                 0.0     & 0.681061 & 0.0      & 0.0 \\
                 0.0     & 0.0      & 0.167854 & 0.0
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                 0.130566 & 0.0      & 0.0       & 0.0 \\
                 0.0      & 0.732227 & 0.0       & 0.0 \\
                 0.0      & 0.0      & 0.985812  & 0.0 \\
                 0.0      & 0.0      & 0.0       & 1.0
                \end{bmatrix}
            \end{equation*}
The generalized singular values computed are 
\[ 
7.593384394490093, 0.930122554989402, 0.17026951585960612, 0.0.
\] 
The computed orthogonal matrices $U$, $V$, $Q$, and the $R$ matrix are: 
            \begin{align*}
                U &= \begin{bmatrix}
                 -0.519777 & 0.747619  & 0.413398 \\
                  0.470025 & 0.654341  &-0.592381 \\
                  0.713378 & 0.113599  & 0.691511
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                  0.259832 &  0.927018   & 0.177229  & -0.20424  \\
                 -0.733955 &  0.0402919  & 0.652334  & -0.184789 \\
                 -0.597084 &  0.369645   &-0.576157  & 0.418206 \\
                 -0.1931.  & -0.0487437  &-0.459449  & -0.865588
                \end{bmatrix} \\
                Q &= \begin{bmatrix}
                 -0.685431 & -0.564405 & -0.459976 & -0.00724571 \\
                  0.681731 & -0.704114 & -0.149854 & -0.130423   \\
                 -0.127188 & -0.380896 &  0.646684 &  0.648491   \\
                 -0.221923 & -0.201466 &  0.589716 & -0.749931  
                \end{bmatrix} \\
                R &= \begin{bmatrix}
                 -3.71474     & -2.42556     & -0.179891   & -0.941672 \\
                 -7.20246e-16 & -9.84284     & -1.8323     & -0.522579 \\
                 -8.91076e-17 &  2.04711e-15 &  6.16149    & -1.43582 \\
                  1.84152e-15 &  1.41087e-15 &  1.2978e-15  & 8.05363 
                \end{bmatrix} 
            \end{align*}
            
We tested residues of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{C}, \tilde{S}$ and $\tilde{R}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{C}\tilde{R}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.41810247019514407 \\ \hline
				$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{S}\tilde{R}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.8941315014073346 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}


\item MATLAB GSVD \eqref{eq:gsvdbymatlab} computed by ``{\tt gsvd(A,B)}'': 

      $m + p = 3+4 \geq n = 4$, $m = 3 < n = 4$ and $p = 4 \leq n = 4$, the structures of $C$ and $S$ are the same as case 1.(a) in Section \ref{def_mat}. 
            
            \begin{equation*}
                C = \begin{bmatrix}
                     0   & 0.1679  &       0  &       0 \\
                     0   &      0  &  0.6811  &       0 \\
                     0   &      0  &       0  &  0.9914
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                    1.0000  &       0  &       0  &       0 \\
                         0  &  0.9858  &       0  &       0 \\
                         0  &       0  &  0.7322  &       0 \\
                         0  &       0  &       0  &  0.1306
                \end{bmatrix}
            \end{equation*}
            The generalized singular values computed are: 
\[ 
0, 0.1703, 0.9301, 7.5934.
\] 
The computed orthogonal matrices $U$, $V$ and the $X$ matrix are given below.
    
            \begin{align*}
                U &= \begin{bmatrix}
                    0.4134 &  -0.7476 &   0.5198 \\
                   -0.5924 &  -0.6543 &  -0.4700 \\
                    0.6915 &  -0.1136 &  -0.7134
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                    0.2042 &   0.1772 &  -0.9270 &  -0.2598 \\
                    0.1848 &   0.6523 &  -0.0403 &   0.7340 \\
                   -0.4182 &  -0.5762 &  -0.3696 &   0.5971 \\
                    0.8656 &  -0.4594 &   0.0487 &   0.1931
                \end{bmatrix} \\
                X &= \begin{bmatrix}
                    0.0584 &  -2.8237  & -6.4020 &  -4.0048 \\
                    1.0504 &  -0.7361  & -7.2732 &   0.6748 \\
                   -5.2227 &   3.0534  & -2.2253 &  -0.6694 \\ 
                    6.0397 &   4.7103  & -1.2944 &  -1.9132
                \end{bmatrix} 
            \end{align*} 
 
We checked the residues of $A$ and $B$ with the computed $\tilde{U}, \tilde{V}, \tilde{X}, \tilde{C}$ and $\tilde{S}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert A - \tilde{U}\tilde{C}\tilde{X}^{T}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 3.5278 \\ \hline
				$res_{B} = \frac{\Vert B - \tilde{V}\tilde{S}\tilde{X}^{T}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.5000 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}
           
\item \textbf{Findings}

\begin{enumerate}
	\item The generalized singular values computed by ``JuliaGSVD'' and ``MATLAB-SVD'' are the same. However, the order are opposite.
	\item The $X$ matrix produced by MATLAB is non-singular.
	\item The eigenvalues of $(A^TA, B^T B)$ computed by MATLAB's function {\tt eig(A'*A,B'*B)} are
		\[
		-1.8035125057805033e^{-15}, \quad
		{\bf 0.028991708031064364}, \quad
		{\bf 0.8651279673000131}, \quad
		{\bf 57.659486562484965}.
		\]

		The square roots are  
		\[
		0.0 + 4.2467781973874066e^{-8}\texttt{im}, \quad
 	 	{\bf 0.17026951585960526}, \quad                  
  	 	{\bf 0.930122554989402},\quad                
   		{\bf 7.593384394490046}.
		\]
		Among these values, eigenvalues {\bf 0.17026951585960526}, {\bf 0.930122554989402} and {\bf 7.593384394490046} are found in the computed gsvs of LAPACK GSVD and MATLAB GSVD. 

\item One of the eigenvalues computed by MATLAB's {\tt eig} is spurious. The eigenvalues of $(A^T A, B^T B)$ computed by MATLAB's function {\tt dsygvic(n, A'*A,B'*B, tol)} are
\[
57.6595, \quad
0.8651, \quad
0.0000, \quad
0.0290.
\]
The square roots are  
\[
7.5934, \quad
0.9301, \quad                  
0.0000,\quad                
0.1703.
\] 
all of which are identical to the computed gsvs of LAPACK-GSVD and MATLAB-GSVD.
\end{enumerate}        

\item Similarly, we test GSVD in Julia 1.3 with the same inputs. 
For the numerical rank, $k = 0$ and $\ell = 4$. 
$D1$ and $D2$ (equivalent to $C$ and $S$ in the proposed version) are:
\begin{align*}
D1  = \begin{bmatrix}
 0.99144 & 0.0 &      0.0 &      0.0 \\
 0.0    &  0.681061 & 0.0 &      0.0 \\
 0.0    &  0.0  &     0.167854 & 0.0
\end{bmatrix}, \ \ \ \
D2 = \begin{bmatrix}
 0.130566 & 0.0     &  0.0  &     0.0 \\
 0.0   &    0.732227 & 0.0  &     0.0 \\
 0.0   &    0.0 &      0.985812 & 0.0 \\
 0.0   &    0.0 &      0.0  &     1.0
\end{bmatrix} 
\end{align*}
The computed orthogonal matrices $U$, $V$, $Q$, the $R0$ matrix (equivalent to $R$ in the proposed version) are: 
\begin{align*}
U & = \begin{bmatrix}
  0.519777 & 0.747619 &  0.413398 \\
 -0.470025 & 0.654341 & -0.592381 \\
 -0.713378 & 0.113599 &  0.691511
\end{bmatrix} \\
V & = \begin{bmatrix}
 -0.259832 &  0.927018  &  0.177229 &  0.20424 \\ 
  0.733955 &  0.0402919 &  0.652334 &  0.184789 \\
  0.597084 &  0.369645  & -0.576157 & -0.418206 \\
  0.1931 &    -0.0487437 & -0.459449 &  0.865588
\end{bmatrix} \\
Q & = \begin{bmatrix}
 -0.685431 & 0.564405 &  0.459976 &  0.00724571 \\
  0.681731 & 0.704114 &  0.149854 &  0.130423  \\
 -0.127188 & 0.380896 & -0.646684 & -0.648491  \\
 -0.221923 & 0.201466 & -0.589716 &  0.749931  
\end{bmatrix} \\
R0 & = \begin{bmatrix}
 3.71474 & -2.42556 & -0.179891 & -0.941672 \\
 0.0     &  9.84284 &  1.8323   &  0.522579 \\
 0.0     &  0.0     &  -6.16149 &   1.43582 \\
 0.0     &  0.0     &  0.0      &  8.05363 
\end{bmatrix} 
\end{align*}
\Red{All these quantities are essentially (up to a sign) the same
with JuliaGSVD.}  

Still, we tested residuals of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{D1}, \tilde{D2}$ and $\tilde{R0}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{D1}\tilde{R0}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.3536371804643456 \\ \hline
				$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{D2}\tilde{R0}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.59375 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}
\end{enumerate}
} 
\end{example} 
    
\newpage
\begin{example} \label{eg:case2b} 
{\rm 
Given a 3-by-5 matrix $A$ and a $4$-by-$5$ matrix $B$ which are rank deficient:
        \begin{displaymath}
            A = \begin{bmatrix}
                1 & 4 & 2 & 3 & 0\\
                3 & 4 & 0 & -2 & 1\\
                4 & 7 & 5 & 6 & 3\\
            \end{bmatrix}, \ \ \ \
            B = \begin{bmatrix}
                1 & 4 & 2 & 3 & 0\\
                2 & 5 & 3 & 4 & 1 \\
                3 & 6 & 4 & 5 & 2\\
                0 & 1 & -1 & 3 & 1
            \end{bmatrix}
        \end{displaymath}
    \begin{enumerate}[(1).]
        \item The LAPACK GSVD \eqref{eq:gsvdbylapack} computed by ``JuliaGSVD'': 
        
            $k = 1, \ell = 3$ and $m = 3 < k+\ell = 4$. Both $B$ and $[A; B]$ are not in full rank, the structures of $C$ and $S$ comply with those of case (2) in Section \ref{def}.
            
            \begin{equation*}
                C = \begin{bmatrix}
                 1.0 & 0.0      & 0.0      & 0.0 \\
                 0.0 & 0.849235 & 0.0      & 0.0 \\
                 0.0 & 0.0      & 0.605834 & 0.0 \\
                \end{bmatrix} , \ \ \ \
                S = \begin{bmatrix}
                 0.0 & 0.528015 & 0.0       & 0.0 \\
                 0.0 & 0.0      & 0.795591  & 0.0 \\
                 0.0 & 0.0      & 0.0       & 1.0 \\
                 0.0 & 0.0      & 0.0       & 0.0
                \end{bmatrix}
            \end{equation*}
The generalized singular values computed are 
\[ 
\texttt{Inf}, 1.6083530545973714, 0.7614900645668164, 0.0.
\] 
The computed orthogonal matrices $U$, $V$, $Q$, and the $R$ matrix are: 
            \begin{align*}
                U &= \begin{bmatrix}
                     -2.22045e-16 &  0.355381  &   -0.934722  \\
                      1.0   &       -1.74736e-16 &  -1.8521e-16 \\
                     -2.2915e-16 &  -0.934722  &   -0.355381  
                \end{bmatrix} \\
                V &= \begin{bmatrix}
                      0.571577  &   -0.711781  &    1.07608e-17 & -0.408248 \\  
                     -0.120069  &   -0.564727  &   -2.13123e-16 &  0.816497 \\  
                     -0.811716  &   -0.417673  &   -1.59451e-16 & -0.408248 \\   
                      1.38917e-16 &  1.22399e-16 & -1.0     &      3.46945e-17
                \end{bmatrix} \\
                Q &= \begin{bmatrix}
                     -0.735494 & -0.356936 & -0.479812 &  0.318474  &  3.59984e-16 \\
                      0.29657 &  -0.540179 &  0.367864 &  0.633716  &  0.288675 \\ 
                      0.130491 &  0.610611 & -0.189162 &  0.700722  & -0.288675  \\ 
                     -0.237256 &  0.432143 &  0.0711454 & 0.0435931 &  0.866025 \\  
                      0.545689 & -0.145639 & -0.770462 &  -0.0637737 &  0.288675 
                \end{bmatrix} \\
                R &= \begin{bmatrix}
                    0.0 & -4.24145 & -0.880735 & 3.33933 & -0.288675 \\
                    0.0 &  0.0  &  2.7394 & -8.38306 & -5.97906 \\
                    0.0 &  0.0  &  -1.77636e-15 & -12.2122 & -8.79399 \\
                    0.0 &  0.0  &  -4.996e-16 & 2.22045e-16 & -3.4641  
                \end{bmatrix}    
            \end{align*}

            We can verify that $R$ has a zero column in the leftmost since $k+l < n$. 

We tested residues of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{C}, \tilde{S}$ and $\tilde{R}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{C}\tilde{R}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.36 \\ \hline
				$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{S}\tilde{R}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.589976856064605 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}

            
\item MATLAB GSVD \eqref{eq:gsvdbymatlab} computed by ``{\tt gsvd(A,B)}'':   
  
      $m + p = 3 + 4 \geq n = 5$ and $m = 3 < n = 5$, $p = 4 < n = 5$, the structures of $C$ and $S$ are the form of case 1.(d) in Section \ref{def_mat}. 
            
            \begin{equation*}
                C = \begin{bmatrix}
                    0 & 0 & 0.8178 & 0 & 0 \\
                    0 & 0 & 0 & 0.9995 & 0 \\
                    0 & 0 & 0 & 0 & 1.0000
                \end{bmatrix}, \ \ \ \
                S = \begin{bmatrix}
                    1.0000 & 0 & 0 & 0 & 0 \\
                    0 & 1.0000 & 0 & 0 & 0 \\
                    0 & 0 & 0.5755 & 0 & 0 \\
                    0 & 0 & 0 & 0.0312 & 0
                \end{bmatrix}
            \end{equation*}
            The generalized singular values computed are 
\[
0, 0, 1.4209, 32.0780, \texttt{Inf}.
\] 
The computed orthogonal matrices $U$, $V$ and the $X$ matrix are:
            \begin{align*}
                U &= \begin{bmatrix}
                   -0.1968 &  0.9805 &  0.0000 \\
                    0.0000 & -0.0000 &  1.0000 \\
                   -0.9805 & -0.1968 & -0.0000
                \end{bmatrix} 
                \\
                V &= \begin{bmatrix}
                   -0.8338  &       0 &   0.3365  &  0.4376 \\
                   -0.5289  &  0.0000 &  -0.2600  & -0.8079 \\
                   -0.1581  &  0.0000 &  -0.9051  &  0.3947 \\
                   -0.0000  & -1.0000 &  -0.0000  & -0.0000 
                \end{bmatrix} 
                \\ 
                X &= \begin{bmatrix}
                   -2.3660  &  0.0000 &  -5.0363  &  0.1935 &   3.0000 \\
                   -6.9285  & -1.0000 &  -9.3550  &  2.5457 &   4.0000 \\
                   -3.8868  &  1.0000 &  -6.4759  &  0.9776 &   0.0000 \\
                   -5.4077  & -3.0000 &  -7.9154  &  1.7617 &  -2.0000 \\
                   -0.8451  & -1.0000 &  -3.5968  & -0.5906 &   1.0000
                \end{bmatrix}
           \end{align*}

We checked the residues of $A$ and $B$ with the computed $\tilde{U}, \tilde{V}, \tilde{X}, \tilde{C}$ and $\tilde{S}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert A - \tilde{U}\tilde{C}\tilde{X}^{T}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.4800 \\ \hline
				$res_{B} = \frac{\Vert B - \tilde{V}\tilde{S}\tilde{X}^{T}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.4000 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}
            
        \item \textbf{Findings}
        
        \begin{enumerate}
        	\item The matrix $X$ in MATLAB GSVD \eqref{eq:gsvdbymatlab} 
      is singular, whose rank is 4.
			\item Neither do the diagonal entries of $C$ and $S$ nor the generalize singular values produced in LAPACK GSVD \eqref{eq:gsvdbylapack} and MATLAB GSVD \eqref{eq:gsvdbymatlab}  share any in common in terms of the number of gsvs and their numerical values. 
			\item The eigenvalues of $(A^TA, B^T B)$ computed by MATLAB's function {\tt eig(A'*A,B'*B)} are		
				\[
					-0.34554912453318243, 
					1.3025318975863486e^{-16},
					{\bf 0.5798671184339763}, 
					{\bf 2.586799548232693}, 
 					\texttt{Inf}. 
				\]

			The square roots are 
				\[
					0.0 + 0.5878342662121547\texttt{im}, 
 					1.1412851955520796e^{-8},            
    				{\bf 0.7614900645668178},             
    				{\bf 1.6083530545973708},              
    				\texttt{Inf}.
				\]
				
			Among these values, eigenvalues {\bf 0.7614900645668178} and {\bf 1.6083530545973708} 
are found in the computed gsvs of LAPACK GSVD. 

			\item \Red{The computation of the eigenvalues of $(A^TA, B^T B)$ by 
MATLAB's function {\tt dsygvic(n, A'*A, B'*B, tol)} threw an error at line 328. I tried to change $k$ to $k(1)$, thus the square roots of the eigenvalues are
				\[
				0.0046, \quad
				0.7710.
				\]
				which makes no sense.}
        \end{enumerate}
        

\item Again, same inputs are tested in Julia 1.3. For the numerical rank determination, $k = 1$ and $\ell = 3$. $D1$ and $D2$ (equivalent to $C$ and $S$ in the proposed version) are:
\begin{align*}
D1 = \begin{bmatrix}
 1.0 & 0.0  &     0.0  &     0.0 \\
 0.0 & 0.849235 & 0.0  &     0.0 \\
 0.0 & 0.0  &  0.605834  & 0.0 
\end{bmatrix}, \ \ \ \
D2 = \begin{bmatrix}
 0.0 & 0.528015 & 0.0 & 0.0 \\
 0.0 & 0.0 & 0.795591 & 0.0 \\
 0.0 & 0.0 & 0.0 & 1.0 \\
 0.0 & 0.0 & 0.0 & 0.0
 \end{bmatrix}
\end{align*}

The computed orthogonal matrices $U$, $V$, $Q$, the $R0$ matrix (equivalent to $R$ in the proposed version) are: 
\begin{align*}
    U &= \begin{bmatrix}
     -2.22045e-16 & -0.355381 & -0.934722 \\  
      1.0    &       1.74736e-16 & -1.8521e-16 \\
     -2.2915e-16  &  0.934722  &   -0.355381  
    \end{bmatrix} \\
    V &= \begin{bmatrix}
     -0.571577  &   -0.711781  &    1.94289e-16 & -0.408248 \\   
      0.120069  &   -0.564727   &   2.35922e-16 &  0.816497  \\
      0.811716  &   -0.417673   &  -1.82146e-17 & -0.408248  \\
      7.69338e-17 &  2.44055e-16 &  1.0     &      3.46945e-17
    \end{bmatrix} \\
    Q &= \begin{bmatrix}
     -0.735494 & -0.356936 & -0.479812 &  -0.318474 &  -1.66533e-16 \\
      0.29657 &  -0.540179 &  0.367864 &  -0.633716 &  -0.288675 \\
      0.130491 &  0.610611 & -0.189162 &  -0.700722 &   0.288675 \\  
     -0.237256 &  0.432143 &  0.0711454 & -0.0435931 & -0.866025 \\  
      0.545689 & -0.145639 & -0.770462  &  0.0637737 & -0.288675  
      \end{bmatrix} \\
    R0 &= \begin{bmatrix}
     0.0 & -4.24145 & -0.880735 & -3.33933 &  0.288675 \\
     0.0 &  0.0     & -2.7394   & -8.38306 & -5.97906 \\
     0.0 &  0.0     &  0.0      & 12.2122  &  8.79399 \\
     0.0 &  0.0     &  0.0      &  0.0  & -3.4641
    \end{bmatrix}
\end{align*}
    It is clear that the leftmost column of $R0$ is all zeros.

\Red{All these quantities are essentially (up to a sign) the same
with JuliaGSVD.}  

Still, we tested residuals of $A$ and $B$ with the computed products $\tilde{U}, \tilde{V}, \tilde{Q}, \tilde{D1}, \tilde{D2}$ and $\tilde{R0}$.

\begin{center}
	\bgroup
	\def\arraystretch{2}% 
		\begin{tabular}{| m{0.25\textwidth}|| c |}
			\hline
    			$res_{A} = \frac{\Vert \tilde{U}^TA\tilde{Q} - \tilde{D1}\tilde{R0}\Vert_1}{max(m,n)\Vert A \Vert_1 \varepsilon}$ & 0.4449492156962062 \\ \hline
				$res_{B} = \frac{\Vert \tilde{V}^TB\tilde{Q} - \tilde{D2}\tilde{R0}\Vert_1}{max(p,n)\Vert B \Vert_1 \varepsilon}$ & 0.305570013362164 \\ 
			\hline
		\end{tabular}
	\egroup
\end{center}

\end{enumerate} 
} \end{example}

% \paragraph{Conclusion.} We find that in all cases, the computed $C$ and $S$ by our proposed version and Julia 1.3 are exactly the same. $U$, $V$, $Q$ and $R$ are mostly the same except for sign difference in certain columns. 
    
% However, the results computed by MATLAB bear less resemblance. On one hand, when the input matrices are of full rank (Case 1 and 3), the diagonal entries of $C$ and $S$, and the generalized singular values produced by MATLAB are the same as those computed by proposed version and Julia 1.3, but in a reversed ordering. On the other hand, when the input matrices are rank deficient (Case 2 and 4), neither the diagonal entries of $C$ and $S$ nor the generalize singular values produced by MATLAB and Julia share anything in common, regardless of numerical values or length. \\
    
    %     \subsubsection{Link between Definition(1) and Definition(2)}
    %     % MATLAB documents the algorithm as follows:
        
    %     % ``The generalized singular value decomposition uses the CS decomposition described in \cite{golub2013matrix}, as well as the built-in \texttt{svd} and \texttt{qr} functions. The CS decomposition is implemented in a local function in the \texttt{gsvd} program file."
        
    %     Definition(1) imposes a constraint on the size of matrix $A$ such that $m \geq n$. To discuss the connection between Definition(1) and Definition(2), we first narrow down Definition(2) to case 1(b) and 1(c) in Section \ref{def_mat}. 
        
    %     In either case, one may verify that the size of $C$ and $S$ are the same as those in Definition(1). Namely, $C$ is $m$-by-$n$ and $S$ is $p$-by-$n$. However, the ordering of diagonal entries in $C$ and $S$ are opposite from each other. 
        
    %     For Definition(1),
    %     \begin{align*}
    %         \begin{pmatrix}
    %              A  \\
    %              B 
    %         \end{pmatrix} = 
    %         \begin{pmatrix}
    %             Q_{1} \\
    %             Q_{2} 
    %         \end{pmatrix}R = 
    %         \begin{pmatrix}
    %             U & 0 \\
    %             0 & V
    %         \end{pmatrix}
    %         \begin{pmatrix}
    %             C  \\
    %             S 
    %         \end{pmatrix}Z^{T}R
    %     \end{align*}
        
    %     Let $X = R^{-1}Z$, then 
        
    %     \begin{align*}
    %         \begin{pmatrix}
    %              A  \\
    %              B 
    %         \end{pmatrix} = 
    %         \begin{pmatrix}
    %             U & 0 \\
    %             0 & V
    %         \end{pmatrix}
    %         \begin{pmatrix}
    %             C  \\
    %             S 
    %         \end{pmatrix}X^{-1}
    %     \end{align*}
        
    %     For Definition (2)
    %     \begin{align*}
    %         \begin{pmatrix}
    %              A  \\
    %              B 
    %         \end{pmatrix} = 
    %         \begin{pmatrix}
    %             Q_{1} \\
    %             Q_{2} 
    %         \end{pmatrix}R = 
    %         \begin{pmatrix}
    %             U & 0 \\
    %             0 & V
    %         \end{pmatrix}
    %         \begin{pmatrix}
    %             \Sigma_1  \\
    %             \Sigma_2 
    %         \end{pmatrix}Z^{T}R =
    %         \begin{pmatrix}
    %             U & 0 \\
    %             0 & V
    %         \end{pmatrix}
    %         \begin{pmatrix}
    %             C  \\
    %             S 
    %         \end{pmatrix}PZ^{T}R
    %     \end{align*}  
        
    %     Let $X = R^{T}ZP^{T}$, then
    %     \begin{align*}
    %         \begin{pmatrix}
    %              A  \\
    %              B 
    %         \end{pmatrix} = 
    %         \begin{pmatrix}
    %             U & 0 \\
    %             0 & V
    %         \end{pmatrix}
    %         \begin{pmatrix}
    %             C  \\
    %             S 
    %         \end{pmatrix}X^{T}
    %     \end{align*}
        
    % Note that if $[A; B]$ is rank deficient, we may use QR decomposition with column pivoting or SVD? \cite{bai1993computing} \cite[pp.~310]{golub2013matrix}
